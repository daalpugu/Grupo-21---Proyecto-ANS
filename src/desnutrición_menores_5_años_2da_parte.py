# -*- coding: utf-8 -*-
"""Desnutrición_Menores_5_Años_2da parte.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KYoWEA35Rm8fI45pYwKqWrg2sFVApHUR

# Desnutrición Aguda en Menores de 5 Años

**Análisis preliminar de los datos**

Registro de pacientes atendidos en las Instituciones Prestadoras de Servicios de Salud con diagnóstico confirmado de Desnutrición Aguda en menores de 5 años y notificados al SIVIGILA desde el año 2016 al 2021.
"""

# Importar librerías
import pandas as pd
import seaborn as sns
import requests
import json
import matplotlib.pyplot as plt

# URL directa del archivo JSON
url = 'https://raw.githubusercontent.com/daalpugu/Grupo-21---Proyecto-ANS/main/data/sivigila_Desnutricion.json'

# Hacer una solicitud GET para obtener el contenido del archivo
response = requests.get(url)

# Cargar el contenido en un objeto JSON
var_des = json.loads(response.text)

# Abrir el archivo de descripción de variables en modo lectura (r) *Para la opción de repositorio local
#with open('https://raw.githubusercontent.com/daalpugu/Grupo-21---Proyecto-ANS/main/data/sivigila_Desnutricion.json', 'r') as file:
#    var_des = json.load(file)

# Convertir el diccionario en DF
df_var_des =  pd.DataFrame(var_des['fields'])

# Permitir que se muestre todo el contenido de la variable
pd.set_option('display.max_colwidth', None)

# Resetear la opción de mostrar todo el contenido de las variables
# pd.reset_option('display.max_colwidth')

df_var_des[['name','description']]

# Cargar la base de datos desde el repositorio en Github
url = 'https://raw.githubusercontent.com/daalpugu/Grupo-21---Proyecto-ANS/main/data/sivigila_desnutricion.csv'
df = pd.read_csv(url)

print(f"El tamaño de la base es: {df.shape}")

# Configurar pandas para mostrar todas las columnas
pd.set_option('display.max_columns', None)

df.head()

df.tail()

# Ajustar las variables decimales, porque hay números con . y otros con ,
df['talla_nac'] = df['talla_nac'].str.replace(',','.')
df['peso_act'] = df['peso_act'].str.replace(',','.')
df['talla_act'] = df['talla_act'].str.replace(',','.')
df['per_braqu'] = df['per_braqu'].str.replace(',','.')

df['talla_nac'] = df['talla_nac'].astype(float)
df['peso_act'] = df['peso_act'].astype(float)
df['talla_act'] = df['talla_act'].astype(float)
df['per_braqu'] = df['per_braqu'].astype(float)

df.tail()

df.info()

# Teniendo en cuenta que la variable edad puede estar en días, meses o años,
# se crea una nueva variable de edad estandarizada en meses para mejor interpretabilidad

# Crear una función para calcular la edad en meses
def edad_meses(fila):
  if fila['uni_med_'] == 1:
    return fila['edad_']*12 # si la edad está en años multiplicar x12 meses
  elif fila['uni_med_'] == 2:
    return fila['edad_']    # si la edad está en meses dejar el mismo valor
  elif fila['uni_med_'] == 3:
    return (round(fila['edad_']/30, 2)) # si la edad está en días, dividir en 30 días y redondear a 2 decimales
  else:
    return None # si no se cumple ninguno de los casos anteriores

# Crear la variable edad_mes
df['edad_mes'] = df.apply(edad_meses, axis=1) # aplicar la función anterior a las filas
df.head()

df['edad_mes'].dtype

# Nuevo df sin variables edad_, uni_med_

df_1 = df.drop(columns=['edad_', 'uni_med_'])
df_1.head()

# Estadísticas descriptivas variables numéricas
descriptivas = df_1.describe(include='all')
descriptivas.T

# Gráfico de barras de las variables: sexo, paciente hospitalizado, esquema de vacunación y año

cols_1 = ['sexo_', 'pac_hos_', 'esq_vac', 'year_', 'tipo_ss_']

n_cols = 5
n_filas = 1
fig,axes = plt.subplots(figsize=(15, 6))

for posicion,variable in enumerate(cols_1):
        aux_lista_valores = df_1[variable] #extrae los valores de la variable
        aux_llaves = aux_lista_valores.unique()
        aux_llaves.sort()
        conteo = []
        for j in aux_llaves:
            aux = aux_lista_valores.tolist().count(j)
            conteo.append(aux)

        plt.subplot(n_filas, n_cols, posicion + 1)
        plt.subplots_adjust(hspace=1.5, wspace= .5) #espacio entre filas
        plt.bar(aux_llaves,conteo, color="darkslateblue")
        plt.title(variable)
        plt.xticks(rotation=45,fontsize=7)
plt.show()

# Gráfico de barras de las variables: sexo, paciente hospitalizado, esquema de vacunación y año

cols_1 = ['t_lechem', 'e_complem', 'esq_vac', 'crec_dllo', 'carne_vac']

n_cols = 5
n_filas = 1
fig,axes = plt.subplots(figsize=(15, 6))

for posicion,variable in enumerate(cols_1):
        aux_lista_valores = df_1[variable] #extrae los valores de la variable
        aux_llaves = aux_lista_valores.unique()
        aux_llaves.sort()
        conteo = []
        for j in aux_llaves:
            aux = aux_lista_valores.tolist().count(j)
            conteo.append(aux)

        plt.subplot(n_filas, n_cols, posicion + 1)
        plt.subplots_adjust(hspace=1.5, wspace= .5) #espacio entre filas
        plt.bar(aux_llaves,conteo, color="darkslateblue")
        plt.title(variable)
        plt.xticks(rotation=45,fontsize=7)
plt.show()

# Histogramas de las variables: 'peso_act', 'talla_act', 'per_braqu' y 'edad_mes'

cols_2 = ['peso_act', 'talla_act', 'crec_dllo', 'edad_mes']

n_cols = 4
n_filas = 1
fig,axes = plt.subplots(n_filas, n_cols, figsize=(15, 6))

for posicion,variable in enumerate(cols_2):
        ax = axes[posicion]
        aux_lista_valores = df_1[variable] #extrae los valores de la variable
        plt.subplot(n_filas, n_cols, posicion + 1)
        plt.subplots_adjust(hspace=1.5) #espacio entre filas
        plt.hist(aux_lista_valores, edgecolor = "k", bins=15, color = 'darkslateblue')
        plt.title(variable)
        xticks = ax.get_xticks()
        interval = max(1, len(xticks) // 10)  # Evitar división por cero
        ax.set_xticks(xticks[::interval])
        ax.set_xticklabels(ax.get_xticks(), rotation=45, fontsize=7)

plt.show()

# Histogramas de las variables: 'peso_nac', 'talla_nac' y 'edad_ges'

cols_2 = ['peso_nac', 'talla_nac', 'edad_ges']

n_cols = 3
n_filas = 1
fig,axes = plt.subplots(n_filas, n_cols, figsize=(15, 6))

for posicion,variable in enumerate(cols_2):
        ax = axes[posicion]
        aux_lista_valores = df_1[variable] #extrae los valores de la variable
        plt.subplot(n_filas, n_cols, posicion + 1)
        plt.subplots_adjust(hspace=1.5) #espacio entre filas
        plt.hist(aux_lista_valores, edgecolor = "k", bins=15, color = 'darkslateblue')
        plt.title(variable)
        xticks = ax.get_xticks()
        interval = max(1, len(xticks) // 10)  # Evitar división por cero
        ax.set_xticks(xticks[::interval])
        ax.set_xticklabels(ax.get_xticks(), rotation=45, fontsize=7)

num_ceros = (df['per_braqu'] == 0).sum()
print(f"Número de valores igual a 0 variable 'per_braqui': {num_ceros}")
num_ceros = (df['peso_nac'] == 0).sum()
print(f"Número de valores igual a 0 variable 'peso_nac': {num_ceros}")
num_ceros = (df['peso_nac'] == 5000).sum()
print(f"Número de valores igual a 5000 variable 'peso_nac': {num_ceros}")
num_ceros = (df['talla_nac'] == 0).sum()
print(f"Número de valores igual a 0 variable 'talla_nac': {num_ceros}")
num_ceros = (df['edad_ges'] == 0).sum()
print(f"Número de valores igual a 0 variable 'edad_ges': {num_ceros}")
num_ceros = ((df['peso_nac'] == 0) & (df['talla_nac'] == 0) & (df['edad_ges'] == 0)).sum()
print(f"Número de registros con 0 en las variables 'edad_ges', 'peso_nac' y 'talla_nac': {num_ceros}")

# Análisis de correlaciones entre variables numéricas:

# Se pasa la variable categórica 'sexo_' a una variable númerica para analizar su correlación con otras
df_1['sexo_'] = df_1['sexo_'].map({'M': 0, 'F': 1})
# Filtrar solo las columnas numéricas
df_numericas = df_1.select_dtypes(include=['float64', 'int64'])
df_numericas = df_numericas.drop(columns=['id', 'semana', 'year_', 'tip_cas_'])

# Calcular la matriz de correlación
matriz_correlacion = df_numericas.corr()

# Configurar el tamaño del gráfico
plt.figure(figsize=(10, 8))

# Crear el mapa de calor con valores numéricos
sns.heatmap(matriz_correlacion, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)

# Mostrar el gráfico
plt.show()

plt.figure(figsize=(12, 6))


# Box plot para peso_act
plt.subplot(1, 3, 1)
sns.boxplot(x=df['peso_act'])
plt.title('Peso Actual')

# Box plot para t_lechem
plt.subplot(1, 3, 2)
sns.boxplot(x=df['t_lechem'])
plt.title('Tiempo Leche Materna')

# Box plot para e_complem_t
plt.subplot(1, 3, 3)
sns.boxplot(x=df['e_complem'])
plt.title('Tiempo Alimentación Complementaria')


plt.tight_layout()
plt.show()

# Eliminar los registros con outliers de las variables 'peso_act', 't_lechem' y 'e_complem'
df_1=df_1[df_1['peso_act'] <= 20]
df_1=df_1[df_1['t_lechem'] <=50]
df_1=df_1[df_1['e_complem'] <= 50]

# Estadísticas descriptivas variables numéricas
descriptivas = df_1.describe(include='all')
descriptivas.T

"""**Completar los valores de las variables que tienen registros en 0**"""

# Extraer las variables que se van a emplear para hacer la regresión lineal: 'peso_act', 'talla_act', 'edad_mes' como variables independientes y 'per_braqu' como
# variable dependiente
df_braq = df_1[['peso_act','talla_act', 'edad_mes', 'per_braqu']]
df_braq

# Organizar los dataframes para aplicar la regresión
X_pred = df_braq[df_braq['per_braqu'] == 0]
X_pred = X_pred.drop('per_braqu', axis=1)

data_model = df_braq[df_braq['per_braqu'] != 0]
X_data = data_model[['peso_act','talla_act', 'edad_mes']]
y_data = data_model[['per_braqu']]
y_data
y_data['per_braqu'].max()

# Se importan las bibliotecas requeridas para aplicar y calibrar un modelo de XGBoost
from sklearn.model_selection import train_test_split
import numpy as np
from xgboost import XGBRegressor
import sklearn.metrics as metrics
from sklearn.model_selection import GridSearchCV, cross_val_score

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.33, random_state=42)

xgb_regressor = XGBRegressor(enable_categorical = True, n_jobs=-1)
xgb_regressor

xgb_regressor.fit(X_train, y_train)
y_pred = xgb_regressor.predict(X_test)


# Métricas del Modelo
rmse_xgboost = np.sqrt(metrics.mean_squared_error(y_pred, y_test))

print('El RMSE del modelo es ', rmse_xgboost)

# Calibración con la estrategia de validación cruzada Grid Search
# Parámetros para aplicar Grid Search
grid_search_params = {
'colsample_bytree': [0.1, 0.3, 0.6, 0.8],
'learning_rate': [0.01, 0.1, 0.2, 0.5],
'n_estimators': [5, 10, 50, 100],
'subsample': [0.1, 0.2, 0.5, 0.7],
'max_depth': [2, 3, 5, 10]}

grid = GridSearchCV(estimator=xgb_regressor, param_grid=grid_search_params, scoring='neg_mean_squared_error',
cv=4, verbose=1)
grid.fit(X_train, y_train)
print("GridSearchCV")
print("Mejores parámetros encontrados: ", grid.best_params_)
print("Menor RMSE encontrado: ", np.sqrt(np.abs(grid.best_score_)))

#Entrenamiento del modelo con los mejores parámetros
xgb_regr_grid = XGBRegressor(colsample_bytree=grid.best_params_['colsample_bytree'],
                             learning_rate=grid.best_params_['learning_rate'],
                             max_depth=grid.best_params_['max_depth'],
                             n_estimators=grid.best_params_['n_estimators'],
                             subsample=grid.best_params_['subsample'],
                             enable_categorical = True, n_jobs=-1)
xgb_regr_grid.fit(X_train, y_train)
y_pred = xgb_regr_grid.predict(X_test)

# Métricas del Modelo
rmse_xgboost_grid = np.sqrt(metrics.mean_squared_error(y_pred, y_test))
print('El RMSE del modelo XGBRegressor calibrado con Grid search es:', rmse_xgboost_grid)

!pip install catboost

# importar librerías
from catboost import CatBoostRegressor

modelCatBoost = CatBoostRegressor(learning_rate=0.1, depth=8, random_seed=42, thread_count=-1, loss_function='RMSE')
modelCatBoost.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50, verbose=100)

y_pred = modelCatBoost.predict(X_test)

# Métricas del Modelo
rmse_CatBoost = np.sqrt(metrics.mean_squared_error(y_pred, y_test))
print('El RMSE del modelo CatBoost es ', rmse_CatBoost)

# Se emplea el modelo de CatBoost, que obtuvo mejor desempeño, para estimar los valores faltantes de 'per_braqu'
y_comp = modelCatBoost.predict(X_pred)
X_pred['per_braqu'] = y_comp
X_pred
df_braq.update(X_pred)
df_braq

# Histogramas de las variables: 'peso_act', 'talla_act', 'per_braqu' y 'edad_mes'

cols_2 = ['peso_act', 'talla_act', 'per_braqu', 'edad_mes']

n_cols = 4
n_filas = 1
fig,axes = plt.subplots(n_filas, n_cols, figsize=(15, 6))

for posicion,variable in enumerate(cols_2):
        ax = axes[posicion]
        aux_lista_valores = df_braq[variable] #extrae los valores de la variable
        plt.subplot(n_filas, n_cols, posicion + 1)
        plt.subplots_adjust(hspace=1.5) #espacio entre filas
        plt.hist(aux_lista_valores, edgecolor = "k", bins=15, color = 'darkslateblue')
        plt.title(variable)
        xticks = ax.get_xticks()
        interval = max(1, len(xticks) // 10)  # Evitar división por cero
        ax.set_xticks(xticks[::interval])
        ax.set_xticklabels(ax.get_xticks(), rotation=45, fontsize=7)

plt.show()

df_1['per_braqu'] = df_braq['per_braqu']
df_1.head()

from scipy import stats

# variables sin ceros
edad_ges_filtrada = df.loc[df['edad_ges'] != 0, 'edad_ges']
peso_nac_filtrada = df.loc[df['peso_nac'] != 0, 'peso_nac']

# Prueba de normalidad - Shapiro-Wilk
stat_edad, p_value_edad = stats.shapiro(edad_ges_filtrada)
stat_peso, p_value_peso = stats.shapiro(peso_nac_filtrada)
print(f'p-value edad gestacional: {p_value_edad} ')
print(f'p-value peso nacimiento: {p_value_peso}')

# qqplot edad gestacional
stats.probplot(edad_ges_filtrada, dist="norm", plot=plt)
plt.title('Q-Q Plot Edad Gestacional')

plt.tight_layout()
plt.show()

# qqplot edad gestacional
stats.probplot(peso_nac_filtrada, dist="norm", plot=plt)
plt.title('Q-Q Plot Peso en el Nacimiento')

plt.tight_layout()
plt.show()

median_ges = df_1.loc[df_1['edad_ges'] != 0, 'edad_ges'].median()
df_1['edad_ges'] = df_1['edad_ges'].replace(0, median_ges)
median_peso = df_1.loc[df_1['peso_nac'] != 0, 'peso_nac'].median()
df_1['peso_nac'] = df_1['peso_nac'].replace(0, median_peso)

df_talla = df_1[['peso_nac', 'edad_ges', 'talla_nac']]
df_talla

X_pred_t = df_talla[df_talla['talla_nac'] == 0]
X_pred_t = X_pred_t.drop('talla_nac', axis=1)

data_model_t = df_talla[df_talla['talla_nac'] != 0]
X_data_t = data_model_t[['peso_nac', 'edad_ges']]
y_data_t = data_model_t[['talla_nac']]
y_data_t['talla_nac'].max()

X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_data_t, y_data_t, test_size=0.33, random_state=42)

xgb_regressor.fit(X_train_t, y_train_t)
y_pred = xgb_regressor.predict(X_test_t)


# Métricas del Modelo
rmse_xgboost = np.sqrt(metrics.mean_squared_error(y_pred, y_test_t))

print('El RMSE del modelo es ', rmse_xgboost)

# Calibración con la estrategia de validación cruzada Grid Search
# Parámetros para aplicar Grid Search
grid_search_params = {
'colsample_bytree': [0.1, 0.3, 0.6, 0.8],
'learning_rate': [0.01, 0.1, 0.2, 0.5],
'n_estimators': [5, 10, 50, 100],
'subsample': [0.1, 0.2, 0.5, 0.7],
'max_depth': [2, 3, 5, 10]}

grid = GridSearchCV(estimator=xgb_regressor, param_grid=grid_search_params, scoring='neg_mean_squared_error',
cv=4, verbose=1)
grid.fit(X_train_t, y_train_t)
print("GridSearchCV")
print("Mejores parámetros encontrados: ", grid.best_params_)
print("Menor RMSE encontrado: ", np.sqrt(np.abs(grid.best_score_)))

xgb_regr_grid = XGBRegressor(colsample_bytree=grid.best_params_['colsample_bytree'],
                             learning_rate=grid.best_params_['learning_rate'],
                             max_depth=grid.best_params_['max_depth'],
                             n_estimators=grid.best_params_['n_estimators'],
                             subsample=grid.best_params_['subsample'],
                             enable_categorical = True, n_jobs=-1)
xgb_regr_grid.fit(X_train, y_train)
y_pred = xgb_regr_grid.predict(X_test)

# Métricas del Modelo
rmse_xgboost_grid = np.sqrt(metrics.mean_squared_error(y_pred, y_test))
print('El RMSE del modelo XGBRegressor calibrado con Grid search es:', rmse_xgboost_grid)

modelCatBoost = CatBoostRegressor(learning_rate=0.01, depth=5, random_seed=42, thread_count=-1, loss_function='RMSE')
modelCatBoost.fit(X_train_t, y_train_t, eval_set=(X_test_t, y_test_t), early_stopping_rounds=50, verbose=100)

y_pred = modelCatBoost.predict(X_test_t)

# Métricas del Modelo
rmse_CatBoost = np.sqrt(metrics.mean_squared_error(y_pred, y_test_t))
print('El RMSE del modelo CatBoost es ', rmse_CatBoost)

y_comp_t = modelCatBoost.predict(X_pred_t)
X_pred_t['talla_nac'] = y_comp_t
X_pred_t
df_talla.update(X_pred_t)
df_talla

# Histogramas de las variables: 'peso_nac', 'talla_nac' y 'edad_ges'

cols_2 = ['peso_nac', 'talla_nac', 'edad_ges']

n_cols = 3
n_filas = 1
fig,axes = plt.subplots(n_filas, n_cols, figsize=(15, 6))

for posicion,variable in enumerate(cols_2):
        ax = axes[posicion]
        aux_lista_valores = df_talla[variable] #extrae los valores de la variable
        plt.subplot(n_filas, n_cols, posicion + 1)
        plt.subplots_adjust(hspace=1.5) #espacio entre filas
        plt.hist(aux_lista_valores, edgecolor = "k", bins=15, color = 'darkslateblue')
        plt.title(variable)
        xticks = ax.get_xticks()
        interval = max(1, len(xticks) // 10)  # Evitar división por cero
        ax.set_xticks(xticks[::interval])
        ax.set_xticklabels(ax.get_xticks(), rotation=45, fontsize=7)

df_1['talla_nac'] = df_talla['talla_nac']
df_1.head()

"""**Implementación de PCA**"""

# Definición del dataframe con las variables que se emplearán para el modelo

df_modelo = df_1[['sexo_', 'comuna', 'tipo_ss_', 'pac_hos_', 'peso_nac', 'talla_nac', 'edad_ges', 't_lechem', 'e_complem', 'carne_vac', 'peso_act', 'talla_act', 'per_braqu', 'edad_mes']]
df_modelo.head()

pip install pca

# Incluir las bibliotecas requeridas
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.decomposition import PCA
from pca import pca as pca_1

# Codificación One-Hot
encoder = OneHotEncoder()
encoded_categ = encoder.fit_transform(df_modelo[['sexo_', 'comuna', 'tipo_ss_', 'pac_hos_', 'carne_vac']]).toarray()
encoded_categ.shape

# Escalado de variables numéricas
scaler = StandardScaler()
scaled_numericas = scaler.fit_transform(df_modelo[['peso_nac', 'talla_nac', 'edad_ges', 't_lechem', 'e_complem', 'peso_act', 'talla_act', 'per_braqu', 'edad_mes']])
scaled_numericas

# Concatenación
combined_model = np.concatenate([scaled_numericas, encoded_categ], axis=1)
columnas = ['peso_nac', 'talla_nac', 'edad_ges', 't_lechem', 'e_complem', 'peso_act', 'talla_act', 'per_braqu', 'edad_mes', 'sex_1', 'sex_2', 'com_1', 'com_2', 'com_3', 'com_4',
            'com_5', 'com_6', 'com_7', 'com_8', 'com_9', 'com_10', 'com_11', 'com_12', 'com_13', 'com_14', 'com_15', 'com_16', 'com_17', 'com_18', 'com_19', 'com_20', 'com_21', 'com_22',
            'com_23', 'com_24', 'com_25', 'com_26', 'com_27', 'com_28','ss_1', 'ss_2', 'ss_3', 'ss_4', 'ss_5', 'ss_6', 'hosp_1', 'hosp_2', 'c_vac_1', 'c_vac_2']
df_model = pd.DataFrame(combined_model, columns=columnas)
df_model

# Aplicación de PCA
pca_skl = PCA()
principal_components = pca_skl.fit_transform(df_model)

# Obtener la varianza explicada
explained_variance = pca_skl.explained_variance_ratio_

# Varianza explicada acumulada
cumulative_variance = np.cumsum(explained_variance)

# Número de componentes principales
n_components = len(explained_variance)
components = np.arange(1, n_components + 1)

# Graficar el Scree Plot
plt.figure(figsize=(14, 6))

# Subplot 1: Scree Plot
plt.subplot(1, 2, 1)
plt.plot(components, explained_variance, 'o-', linewidth=2, color='blue')
plt.title('Varianza Explicada')
plt.xlabel('Número de Componentes Principales', fontsize=14)
plt.ylabel('Varianza Explicada', fontsize=14)
plt.yticks(fontsize=10)
plt.xticks(components, rotation=90, fontsize=10)
plt.grid(True)

# Subplot 2: Varianza Explicada Acumulada
plt.subplot(1, 2, 2)
plt.plot(components, cumulative_variance, 'o-', linewidth=2, color='green')
plt.title('Varianza Explicada Acumulada')
plt.xlabel('Número de Componentes Principales', fontsize=14)
plt.ylabel('Varianza Explicada Acumulada', fontsize=14)
plt.yticks(fontsize=10)
plt.xticks(components, rotation=90, fontsize=10)
plt.grid(True)

plt.tight_layout()
plt.show()

# Aplicar el criterio de Kaiser
varianza_expl = pca_skl.explained_variance_
num_components = sum(varianza_expl > 1)
print("Número de componentes según Kaiser:", num_components)

"""Puesto que las 3 primeras variables que sugiere el método de Kaiser explican algo menos del 60% de la varianza acumulada, se opta por tomar como guía las gráficas de varianza explicada y varianza explicada acumulada, tomando como referencia el siguiente valor por encima del 90% de varianza acumulada explicada, el cual se supera con los primeros 12 componentes principales."""

# Explicación del efecto de las variables en los dos primeros componentes principales
# Inicializamos el objeto PCA
model = pca_1(n_components=12)

# Ajustamos el PCA a nuestros datos
gasto_pca = model.fit_transform(df_model, verbose=False)

# Creamos el biplot
fig, ax = model.biplot(legend=False, color_arrow="red", arrowdict={'color_text':"red"})

df_modelo_pca = pd.DataFrame(principal_components[:, :12])
df_modelo_pca

"""**Clustering**

**K-Medias**
"""

# Incluimos las bibliotecas requeridas
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

varianza_intra_cluster = []
silhouettes = []
Y = {}
for k in range (1, 11): # Evaluamos entre 1 a 11 clusters posibles
    kmeans = KMeans(n_clusters = k, random_state = 123, n_init=10).fit(df_modelo_pca)
    varianza_intra_cluster.append(kmeans.inertia_)
    try:
        silhouette = silhouette_score(df_modelo_pca, kmeans.labels_)
    except:
        silhouette = 0 # El índice de Silhouette sólo se puede calcular cuando se tiene K>1
    silhouettes.append(silhouette)
    Y[k] = kmeans.labels_

plt.plot(range(1, 11), varianza_intra_cluster, marker='o')
plt.xlabel('Número de clústeres (K)', fontsize=14)
plt.ylabel('Varianza intra clúster', fontsize=14)
plt.yticks(fontsize=12)
plt.xticks(fontsize=12)
plt.show()

plt.plot(range(1, 11), silhouettes, marker='o')
plt.xlabel('Número de clústeres (K)', fontsize=14)
plt.ylabel('Índice de Silhouette', fontsize=14)
plt.yticks(fontsize=12)
plt.xticks(fontsize=12)
plt.show()

kmeans = KMeans(n_clusters = 3, random_state = 123, n_init=10).fit_predict(df_modelo_pca)
fig, ax = plt.subplots()
g_puntos = plt.scatter(df_modelo_pca.iloc[:, 0], df_modelo_pca.iloc[:, 1], c = kmeans, alpha = 0.8, label = "Centroides")
plt.show()

"""**DBSCAN**"""

!pip install kneed

#Cargar las librerías a utilizar
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors
from kneed import KneeLocator

neigh = NearestNeighbors(n_neighbors = 10)
nbrs = neigh.fit(df_modelo_pca)
distancias, indices = nbrs.kneighbors(df_modelo_pca)
distancias = np.sort(distancias.flatten())
fig=plt.figure(figsize=(10,8), dpi= 100, facecolor='w', edgecolor='k')
plt.axhline(y = 2, color = 'r', linestyle = '--')
plt.plot(distancias)

i = np.arange(len(distancias))
knee = KneeLocator(i, distancias, S=1, curve='convex', direction='increasing', interp_method='polynomial')

print(distancias[knee.knee])

db = DBSCAN(eps=distancias[knee.knee], min_samples=24)
clusters=db.fit_predict(df_modelo_pca)

fig, ax = plt.subplots(figsize=(10,8), dpi= 100, facecolor='w', edgecolor='k')
g = plt.scatter(df_modelo_pca.iloc[:, 0], df_modelo_pca.iloc[:, 1], c = clusters, alpha = 0.8)
legend = ax.legend(*g.legend_elements(), loc = "upper right", title = "Clusteres \n teóricos")
ax.add_artist(legend)
plt.show()

"""**PCA para las variables númericas**"""

columnas_num = ['peso_nac', 'talla_nac', 'edad_ges', 't_lechem', 'e_complem', 'peso_act', 'talla_act', 'per_braqu', 'edad_mes']
df_model_num = pd.DataFrame(scaled_numericas, columns=columnas_num)
df_model_num

# Aplicación de PCA
principal_components = pca_skl.fit_transform(df_model_num)

# Obtener la varianza explicada
explained_variance = pca_skl.explained_variance_ratio_

# Varianza explicada acumulada
cumulative_variance = np.cumsum(explained_variance)

# Número de componentes principales
n_components = len(explained_variance)
components = np.arange(1, n_components + 1)

# Graficar el Scree Plot
plt.figure(figsize=(14, 6))

# Subplot 1: Scree Plot
plt.subplot(1, 2, 1)
plt.plot(components, explained_variance, 'o-', linewidth=2, color='blue')
plt.title('Varianza Explicada')
plt.xlabel('Número de Componentes Principales', fontsize=14)
plt.ylabel('Varianza Explicada', fontsize=14)
plt.yticks(fontsize=10)
plt.xticks(components, rotation=90, fontsize=10)
plt.grid(True)

# Subplot 2: Varianza Explicada Acumulada
plt.subplot(1, 2, 2)
plt.plot(components, cumulative_variance, 'o-', linewidth=2, color='green')
plt.title('Varianza Explicada Acumulada')
plt.xlabel('Número de Componentes Principales', fontsize=14)
plt.ylabel('Varianza Explicada Acumulada', fontsize=14)
plt.yticks(fontsize=10)
plt.xticks(components, rotation=90, fontsize=10)
plt.grid(True)

plt.tight_layout()
plt.show()

# Aplicar el criterio de Kaiser
varianza_expl = pca_skl.explained_variance_
num_components = sum(varianza_expl > 1)
print("Número de componentes según Kaiser:", num_components)

# Explicación del efecto de las variables en los dos primeros componentes principales
# Inicializamos el objeto PCA
model = pca_1(n_components=5)

# Ajustamos el PCA a nuestros datos
gasto_pca = model.fit_transform(df_model_num, verbose=False)

# Creamos el biplot
fig, ax = model.biplot(legend=False, color_arrow="red", arrowdict={'color_text':"red"})

df_modelo_pca = pd.DataFrame(principal_components[:, :5])
df_modelo_pca

"""**Clustering con el nuevo dataframe de PCA de las variables numéricas**

**K-Medias**
"""

varianza_intra_cluster = []
silhouettes = []
Y = {}
for k in range (1, 11): # Evaluamos entre 1 a 11 clusters posibles
    kmeans = KMeans(n_clusters = k, random_state = 123, n_init=10).fit(df_modelo_pca)
    varianza_intra_cluster.append(kmeans.inertia_)
    try:
        silhouette = silhouette_score(df_modelo_pca, kmeans.labels_)
    except:
        silhouette = 0 # El índice de Silhouette sólo se puede calcular cuando se tiene K>1
    silhouettes.append(silhouette)
    Y[k] = kmeans.labels_

plt.plot(range(1, 11), varianza_intra_cluster, marker='o')
plt.xlabel('Número de clústeres (K)', fontsize=14)
plt.ylabel('Varianza intra clúster', fontsize=14)
plt.yticks(fontsize=12)
plt.xticks(fontsize=12)
plt.show()

plt.plot(range(1, 11), silhouettes, marker='o')
plt.xlabel('Número de clústeres (K)', fontsize=14)
plt.ylabel('Índice de Silhouette', fontsize=14)
plt.yticks(fontsize=12)
plt.xticks(fontsize=12)
plt.show()

kmeans = KMeans(n_clusters = 3, random_state = 123, n_init=10).fit_predict(df_modelo_pca)
fig, ax = plt.subplots()
g_puntos = plt.scatter(df_modelo_pca.iloc[:, 0], df_modelo_pca.iloc[:, 1], c = kmeans, alpha = 0.8, label = "Centroides")
plt.show()

"""**DBSCAN**"""

neigh = NearestNeighbors(n_neighbors = 10)
nbrs = neigh.fit(df_modelo_pca)
distancias, indices = nbrs.kneighbors(df_modelo_pca)
distancias = np.sort(distancias.flatten())
fig=plt.figure(figsize=(10,8), dpi= 100, facecolor='w', edgecolor='k')
plt.axhline(y = 1, color = 'r', linestyle = '--')
plt.plot(distancias)

i = np.arange(len(distancias))
knee = KneeLocator(i, distancias, S=1, curve='convex', direction='increasing', interp_method='polynomial')

print(distancias[knee.knee])

db = DBSCAN(eps=distancias[knee.knee], min_samples=8)
clusters=db.fit_predict(df_modelo_pca)

fig, ax = plt.subplots(figsize=(10,8), dpi= 100, facecolor='w', edgecolor='k')
g = plt.scatter(df_modelo_pca.iloc[:, 0], df_modelo_pca.iloc[:, 1], c = clusters, alpha = 0.8)
legend = ax.legend(*g.legend_elements(), loc = "upper right", title = "Clusteres \n teóricos")
ax.add_artist(legend)
plt.show()